{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7edb1b80-dc5e-4770-b83f-8dfd39a9ab11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d931cc7-9815-43b1-a339-e54fbaf38dc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def f_get_secret(key):\n",
    "    try:\n",
    "        return dbutils.secrets.get(scope=\"cryptoproject\", key=key)\n",
    "    except Exception as e:\n",
    "        raise(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2311bfc-17ae-491b-8afb-9da646eb7d09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def f_add_input_file_name_loadtime(df,date_part=None,file_name=None):\n",
    "    from pyspark.sql.functions import input_file_name,split,current_timestamp,lit,to_date,regexp_extract\n",
    "    try:\n",
    "        if(file_name is None and date_part is None):\n",
    "            df_final=df.select(\"*\").\\\n",
    "                withColumn(\"input_file_name\",input_file_name()).\\\n",
    "                withColumn(\"network\", regexp_extract(input_file_name(), r\"unstructured_data/(.*?)/txn_date\", 1)). \\\n",
    "                withColumn(\"txn_date\", regexp_extract(input_file_name(), r\"txn_date=([0-9\\-]+)\", 1)). \\\n",
    "                withColumn(\"load_timestamp\",current_timestamp())\n",
    "        # elif(file_name is None):\n",
    "        #     df_final=df.select(\"*\").\\\n",
    "        #         withColumn(\"input_file_name\",split(input_file_name(),'/')[4]).\\\n",
    "        #         withColumn(\"date_part\",lit(date_part)).\\\n",
    "        #         withColumnRenamed(\"date_part\",to_date(date_part)).\\\n",
    "        #         withColumn(\"load_timestamp\",current_timestamp())\n",
    "        # else:\n",
    "        #     df_final=df.select(\"*\").\\\n",
    "        #         withColumn(\"input_file_name\",lit(file_name)).\\\n",
    "        #         withColumn(\"date_part\",to_date(lit(date_part))).\\\n",
    "        #         withColumn(\"load_timestamp\",current_timestamp())\n",
    "        return df_final\n",
    "    except Exception as e:\n",
    "        raise(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec651b3f-fc6a-446e-99d2-b93ad3a10751",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def f_validate_schema(df,sink_schema):\n",
    "    no_rows=df.count()\n",
    "    if(no_rows<=100000):\n",
    "       no_files=1\n",
    "    elif(no_rows>100000 and no_rows<=1000000):\n",
    "       no_files=3\n",
    "    elif(no_rows>1000000 and no_rows<=10000000):\n",
    "       no_files=5\n",
    "    source_schema=df.limit(1).dtypes\n",
    "    if(source_schema==sink_schema):\n",
    "        return no_files\n",
    "    else:\n",
    "        raise Exception(\"Schema is not matched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5b2a8a2-660d-430b-b880-1366bb67ce9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def f_get_primary_key(df):\n",
    "    try:\n",
    "        import mack as mk\n",
    "        primary_key=mk.find_composite_key_candidates(df)\n",
    "        merge_condition=\" \"\n",
    "        for i in range(len(primary_key)):\n",
    "            if(i==len(primary_key)-1):\n",
    "                merge_condition+=\"tgt.\"+primary_key[i]+\"=src.\"+primary_key[i]\n",
    "            else:\n",
    "                merge_condition+=\"tgt.\"+primary_key[i]+\"=src.\"+primary_key[i]+\" AND \"\n",
    "        return (merge_condition)\n",
    "    except Exception as err:\n",
    "        print(\"Error occured \",str(err))\n",
    "        raise err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad5f8c19-321f-4990-807a-4092b4304f9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def f_merge_delta_data(input_df, catalog_name, schema_name, table_name, merge_condition, partition_column):\n",
    "  spark.conf.set(\"spark.databricks.optimizer.dynamicPartitionPruning\",\"true\")\n",
    "  # dynamic: use merge_condition  A.partition_column = B.partition_column \n",
    "  # if use merge_condition  A.partition_column = 'value', don'n need set dynamic is true\n",
    "  try:\n",
    "    target_table = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "    from delta.tables import DeltaTable\n",
    "    if (spark._jsparkSession.catalog().tableExists(target_table)):\n",
    "      deltaTable = DeltaTable.forName(spark, target_table)\n",
    "      deltaTable.alias(\"tgt\").merge(\n",
    "          input_df.alias(\"src\"),\n",
    "          merge_condition) \\\n",
    "        .whenMatchedUpdateAll()\\\n",
    "        .whenNotMatchedInsertAll()\\\n",
    "        .execute()\n",
    "    else:\n",
    "      input_df.write.mode(\"overwrite\").partitionBy(partition_column).format(\"delta\").saveAsTable(target_table)\n",
    "  except Exception as err:\n",
    "      print(\"Error occured \",str(err))\n",
    "      raise err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14eb0da6-7642-4437-94b2-64b392476055",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_ingestion_date(input_df):\n",
    "  output_df = input_df.withColumn(\"ingestion_date\", current_timestamp())\n",
    "  return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a83580a-63a5-4d5d-b49d-3e64654c17a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def re_arrange_partition_column(input_df, partition_column):\n",
    "  column_list = []\n",
    "  for column_name in input_df.schema.names:\n",
    "    if column_name != partition_column:\n",
    "      column_list.append(column_name)\n",
    "  column_list.append(partition_column)\n",
    "  output_df = input_df.select(column_list)\n",
    "  return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bed0dbc-1b62-4fef-892c-dc424b0be9f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def overwrite_partition(input_df, db_name, table_name, partition_column):\n",
    "  output_df = re_arrange_partition_column(input_df, partition_column)\n",
    "  spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"dynamic\")\n",
    "  if (spark._jsparkSession.catalog().tableExists(f\"{db_name}.{table_name}\")):\n",
    "    output_df.write.mode(\"overwrite\").insertInto(f\"{db_name}.{table_name}\")\n",
    "  else:\n",
    "    output_df.write.mode(\"overwrite\").partitionBy(partition_column).format(\"parquet\").saveAsTable(f\"{db_name}.{table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f1e218c-4acb-40aa-b501-761cfcecca44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def df_column_to_list(input_df, column_name):\n",
    "  df_row_list = input_df.select(column_name) \\\n",
    "                        .distinct() \\\n",
    "                        .collect()\n",
    "  \n",
    "  column_value_list = [row[column_name] for row in df_row_list]\n",
    "  return column_value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b179db6e-6437-47bb-b48b-6761865e1d77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def merge_delta_data(input_df, db_name, table_name, folder_path, merge_condition, partition_column):\n",
    "  spark.conf.set(\"spark.databricks.optimizer.dynamicPartitionPruning\",\"true\")\n",
    "  # dynamic: use merge_condition  A.partition_column = B.partition_column \n",
    "  # if use merge_condition  A.partition_column = 'value', don'n need set dynamic is true\n",
    "  from delta.tables import DeltaTable\n",
    "  if (spark._jsparkSession.catalog().tableExists(f\"{db_name}.{table_name}\")):\n",
    "    deltaTable = DeltaTable.forPath(spark, f\"{folder_path}/{table_name}\")\n",
    "    deltaTable.alias(\"tgt\").merge(\n",
    "        input_df.alias(\"src\"),\n",
    "        merge_condition) \\\n",
    "      .whenMatchedUpdateAll()\\\n",
    "      .whenNotMatchedInsertAll()\\\n",
    "      .execute()\n",
    "  else:\n",
    "    input_df.write.mode(\"overwrite\").partitionBy(partition_column).format(\"delta\").saveAsTable(f\"{db_name}.{table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8866c8c4-605c-4f0d-93ea-1fa39227086c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def get_access_token():\n",
    "    url = \"https://open.larksuite.com/open-apis/auth/v3/tenant_access_token/internal\"\n",
    "    payload = json.dumps({\n",
    "\t\"app_id\": os.getenv(\"LARK_APP_ID\"),\n",
    "\t\"app_secret\": os.getenv(\"LARK_APP_SECRET\")\n",
    "    })\n",
    "\n",
    "    headers = {\n",
    "    'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "    return response.json().get(\"tenant_access_token\")\n",
    "\n",
    "def f_push_alert_message(exception_message, context): \n",
    "    notebook_path = context.get(\"extraContext\").get(\"notebook_path\")\n",
    "    notebook_link = \"dbc-b211122e-383d.cloud.databricks.com/editor/notebooks/\" + str(context.get(\"extraContext\").get(\"notebook_id\"))\n",
    "    workflow = context.get(\"tags\").get(\"jobName\")\n",
    "\n",
    "    message = f'[ERROR] Notebook execution failure\\nNotebook: {notebook_path}\\nNotebook link: {notebook_link}\\nWorkflow: {workflow}\\n{exception_message}'\n",
    "\n",
    "    url = \"https://open.larksuite.com/open-apis/im/v1/messages?receive_id_type=chat_id\"\n",
    "    content = {'type':'text','content': message}\n",
    "    content_json = {\n",
    "        content['type']: content['content']\n",
    "    }\n",
    "\n",
    "    req_body = {\n",
    "        \"msg_type\": \"text\",\n",
    "        \"receive_id\": os.getenv(\"LARK_RECIEVER_ID\"),\n",
    "        \"content\": json.dumps(content_json)\n",
    "    }\n",
    "\n",
    "    payload = json.dumps(req_body)\n",
    "\n",
    "    token = get_access_token()\n",
    "    headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': f'Bearer {token}'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67666e12-58f8-491d-94a8-e9f4fd6f44de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def f_get_configured_logger(context):\n",
    "    log4jLogger = spark.sparkContext._jvm.org.apache.log4j \n",
    "    notebook_path = context.get(\"extraContext\").get(\"notebook_path\")\n",
    "\n",
    "    logger = log4jLogger.LogManager.getLogger(\"CustomLogs\") \n",
    "    logger.info(f\"Starting logging for notebook {notebook_path}\")\n",
    "    # dbfs:/cluster-logs/<cluster_id>/driver/log4j-active.log\n",
    "    return logger"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 877050942087000,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "common_functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
